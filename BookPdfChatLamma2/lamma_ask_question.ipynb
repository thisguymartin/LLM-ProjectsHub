{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamma 2+ Pinecone + Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if this is the first time running the notebook so you can install dependencies to your local machine in your enviroment\n",
    "\n",
    "  # !pip install langchain\n",
    "  # !pip install pypdf\n",
    "  # !pip install unstructured\n",
    "  # !pip install sentence_transformers\n",
    "  # !pip install pinecone-client\n",
    "  # !pip install llama-cpp-python\n",
    "  # !pip install huggingface_hub\n",
    "  # !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import pinecone\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = OnlinePDFLoader(\"https://falksangdata.no/wp-content/uploads/2022/11/DataScience4dummies.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2333"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39804f3d-5bde-4dd0-afd7-889ca6c29fd3\n",
      "us-west4-gcp-free\n"
     ]
    }
   ],
   "source": [
    "print(PINECONE_API_KEY)\n",
    "print(PINECONE_API_ENV)\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"langchainpinecone\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings for Each of the Text Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run once to create the index.\n",
    "# docsearch=Pinecone.from_texts([t.page_content for t in docs], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is the BigO?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search(query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Data science, machine learning engineering, and data engineering cover different functions within the big data paradigm — an approach wherein huge velocities, varieties, and volumes of structured, unstructured, and semistructured data are being captured, processed, stored, and analyzed using a set of techniques and technologies that are completely novel compared to those that were used in decades past.', metadata={})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Docs to get the Answer Back (Llama 2 Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using pip 23.2.1 from /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/pip (python 3.11)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.1.77.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l  Running command pip subprocess to install build dependencies\n",
      "  Collecting setuptools>=42\n",
      "    Obtaining dependency information for setuptools>=42 from https://files.pythonhosted.org/packages/c7/42/be1c7bbdd83e1bfb160c94b9cafd8e25efc7400346cf7ccdbdb452c467fa/setuptools-68.0.0-py3-none-any.whl.metadata\n",
      "    Using cached setuptools-68.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Collecting scikit-build>=0.13\n",
      "    Obtaining dependency information for scikit-build>=0.13 from https://files.pythonhosted.org/packages/fa/af/b3ef8fe0bb96bf7308e1f9d196fc069f0c75d9c74cfaad851e418cc704f4/scikit_build-0.17.6-py3-none-any.whl.metadata\n",
      "    Using cached scikit_build-0.17.6-py3-none-any.whl.metadata (14 kB)\n",
      "  Collecting cmake>=3.18\n",
      "    Obtaining dependency information for cmake>=3.18 from https://files.pythonhosted.org/packages/8a/2b/042528c0ce76d53ea4dfa305048e4f9914954421188c1f006c15b2814a0b/cmake-3.27.0-py2.py3-none-macosx_10_10_universal2.macosx_10_10_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl.metadata\n",
      "    Using cached cmake-3.27.0-py2.py3-none-macosx_10_10_universal2.macosx_10_10_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl.metadata (6.7 kB)\n",
      "  Collecting ninja\n",
      "    Using cached ninja-1.11.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (270 kB)\n",
      "  Collecting distro (from scikit-build>=0.13)\n",
      "    Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "  Collecting packaging (from scikit-build>=0.13)\n",
      "    Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
      "    Obtaining dependency information for wheel>=0.32.0 from https://files.pythonhosted.org/packages/17/11/f139e25018ea2218aeedbedcf85cd0dd8abeed29a38ac1fda7f5a8889382/wheel-0.41.0-py3-none-any.whl.metadata\n",
      "    Using cached wheel-0.41.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "  Using cached setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
      "  Using cached scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
      "  Using cached cmake-3.27.0-py2.py3-none-macosx_10_10_universal2.macosx_10_10_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (47.4 MB)\n",
      "  Using cached wheel-0.41.0-py3-none-any.whl (64 kB)\n",
      "  Installing collected packages: ninja, cmake, wheel, setuptools, packaging, distro, scikit-build\n",
      "  Successfully installed cmake-3.27.0 distro-1.8.0 ninja-1.11.1 packaging-23.1 scikit-build-0.17.6 setuptools-68.0.0 wheel-0.41.0\n",
      "\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l  Running command Getting requirements to build wheel\n",
      "  running egg_info\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l  Running command Preparing metadata (pyproject.toml)\n",
      "  running dist_info\n",
      "  creating /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python.egg-info\n",
      "  writing /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python.egg-info/top_level.txt\n",
      "  writing manifest file '/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file '/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  creating '/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-modern-metadata-7q3zss7c/llama_cpp_python-0.1.77.dist-info'\n",
      "\u001b[?25hdone\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "  Link requires a different Python (3.11.4 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/3a/be/650f9c091ef71cb01d735775d554e068752d3ff63d7943b26316dc401749/numpy-1.21.2.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.4 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/5f/d6/ad58ded26556eaeaa8c971e08b6466f17c4ac4d786cd3d800e26ce59cc01/numpy-1.21.3.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.4 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/fb/48/b0708ebd7718a8933f0d3937513ef8ef2f4f04529f1f66ca86d873043921/numpy-1.21.4.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.4 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/c2/a8/a924a09492bdfee8c2ec3094d0a13f2799800b4fdc9c890738aeeb12c72e/numpy-1.21.5.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.4 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/45/b7/de7b8e67f2232c26af57c205aaad29fe17754f793404f59c8a730c7a191a/numpy-1.21.6.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/48/73/df07644e8fa1127a7985db70cf1d07123004e2dd7a3cf33e8b83297a775b/numpy-1.25.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.25.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.25.1-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "\n",
      "\n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "    CMake.\n",
      "\n",
      "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "    CMake that the project does not need compatibility with older versions.\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "\n",
      "  -- The C compiler identification is AppleClang 14.0.3.14030022\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- The CXX compiler identification is AppleClang 14.0.3.14030022\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Configuring done (0.8s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_cmake_test_compile/build\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja' generator - success\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "  Configuring Project\n",
      "    Working directory:\n",
      "      /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_skbuild/macosx-13.0-arm64-3.11/cmake-build\n",
      "    Command:\n",
      "      /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-build-env-dpzs49mt/overlay/lib/python3.11/site-packages/cmake/data/bin/cmake /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-build-env-dpzs49mt/overlay/lib/python3.11/site-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_skbuild/macosx-13.0-arm64-3.11/cmake-install -DPYTHON_VERSION_STRING:STRING=3.11.4 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-build-env-dpzs49mt/overlay/lib/python3.11/site-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai/bin/python -DPYTHON_INCLUDE_DIR:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai/include/python3.11 -DPYTHON_LIBRARY:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai/lib/libpython3.11.dylib -DPython_EXECUTABLE:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai/bin/python -DPython_ROOT_DIR:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai/include/python3.11 -DPython3_EXECUTABLE:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai/bin/python -DPython3_ROOT_DIR:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/Users/martinpatino/anaconda3/envs/langchain_ai/include/python3.11 -DCMAKE_MAKE_PROGRAM:FILEPATH=/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-build-env-dpzs49mt/overlay/lib/python3.11/site-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_OSX_DEPLOYMENT_TARGET:STRING=13.0 -DCMAKE_OSX_ARCHITECTURES:STRING=arm64 -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "  -- The C compiler identification is AppleClang 14.0.3.14030022\n",
      "  -- The CXX compiler identification is AppleClang 14.0.3.14030022\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.39.2 (Apple Git-143)\")\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:116 (message):\n",
      "    Git repository not found; to enable automatic generation of build info,\n",
      "    make sure Git is installed and the project is a Git repository.\n",
      "\n",
      "\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Accelerate framework found\n",
      "  -- Could not find nvcc, please set CUDAToolkit_ROOT.\n",
      "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:283 (message):\n",
      "    cuBLAS not found\n",
      "\n",
      "\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: arm64\n",
      "  -- ARM detected\n",
      "  -- Configuring done (0.6s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_skbuild/macosx-13.0-arm64-3.11/cmake-build\n",
      "  [1/7] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
      "  [2/7] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
      "  [3/7] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
      "  [4/7] Linking C static library vendor/llama.cpp/libggml_static.a\n",
      "  [5/7] Linking C shared library vendor/llama.cpp/libggml_shared.dylib\n",
      "  [6/7] Linking CXX shared library vendor/llama.cpp/libllama.dylib\n",
      "  [6/7] Install the project...\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_skbuild/macosx-13.0-arm64-3.11/cmake-install/lib/libggml_shared.dylib\n",
      "  -- Installing: /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_skbuild/macosx-13.0-arm64-3.11/cmake-install/lib/libllama.dylib\n",
      "  -- Installing: /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_skbuild/macosx-13.0-arm64-3.11/cmake-install/bin/convert.py\n",
      "  -- Installing: /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_skbuild/macosx-13.0-arm64-3.11/cmake-install/bin/convert-lora-to-ggml.py\n",
      "  -- Installing: /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-install-3npkufzf/llama-cpp-python_bc960e8d5fa442228b6deb978c5bb57b/_skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/libllama.dylib\n",
      "\n",
      "  copying llama_cpp/llama_types.py -> _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/llama_types.py\n",
      "  copying llama_cpp/__init__.py -> _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/__init__.py\n",
      "  copying llama_cpp/llama_cpp.py -> _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/llama_cpp.py\n",
      "  copying llama_cpp/llama.py -> _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/llama.py\n",
      "  creating directory _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/server\n",
      "  copying llama_cpp/server/__init__.py -> _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/server/__init__.py\n",
      "  copying llama_cpp/server/app.py -> _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/server/app.py\n",
      "  copying llama_cpp/server/__main__.py -> _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/server/__main__.py\n",
      "\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/llama_types.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/__init__.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/llama.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/server\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/server/__init__.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/server\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/server/app.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/server\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/server/__main__.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/server\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/llama_cpp/libllama.dylib -> _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp\n",
      "  copied 7 files\n",
      "  running build_ext\n",
      "  installing to _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/llama_types.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/__init__.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/llama_cpp.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp/server\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/server/__init__.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp/server\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/server/app.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp/server\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/server/__main__.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp/server\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/libllama.dylib -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/setuptools/lib.macosx-13.0-arm64-cpython-311/llama_cpp/llama.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp\n",
      "  copied 8 files\n",
      "  running install_data\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.data\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.data/data\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.data/data/lib\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/lib/libggml_shared.dylib -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.data/data/lib\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/lib/libllama.dylib -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.data/data/lib\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.data/data/bin\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/bin/convert.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.data/data/bin\n",
      "  copying _skbuild/macosx-13.0-arm64-3.11/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.data/data/bin\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Copying llama_cpp_python.egg-info to _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77-py3.11.egg-info\n",
      "  running install_scripts\n",
      "  copied 0 files\n",
      "  creating _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel/llama_cpp_python-0.1.77.dist-info/WHEEL\n",
      "  creating '/private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-wheel-qbcwvndc/.tmp-sax7nah6/llama_cpp_python-0.1.77-cp311-cp311-macosx_13_0_arm64.whl' and adding '_skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel' to it\n",
      "  adding 'llama_cpp/__init__.py'\n",
      "  adding 'llama_cpp/libllama.dylib'\n",
      "  adding 'llama_cpp/llama.py'\n",
      "  adding 'llama_cpp/llama_cpp.py'\n",
      "  adding 'llama_cpp/llama_types.py'\n",
      "  adding 'llama_cpp/server/__init__.py'\n",
      "  adding 'llama_cpp/server/__main__.py'\n",
      "  adding 'llama_cpp/server/app.py'\n",
      "  adding 'llama_cpp_python-0.1.77.data/data/bin/convert-lora-to-ggml.py'\n",
      "  adding 'llama_cpp_python-0.1.77.data/data/bin/convert.py'\n",
      "  adding 'llama_cpp_python-0.1.77.data/data/lib/libggml_shared.dylib'\n",
      "  adding 'llama_cpp_python-0.1.77.data/data/lib/libllama.dylib'\n",
      "  adding 'llama_cpp_python-0.1.77.dist-info/LICENSE.md'\n",
      "  adding 'llama_cpp_python-0.1.77.dist-info/METADATA'\n",
      "  adding 'llama_cpp_python-0.1.77.dist-info/WHEEL'\n",
      "  adding 'llama_cpp_python-0.1.77.dist-info/top_level.txt'\n",
      "  adding 'llama_cpp_python-0.1.77.dist-info/RECORD'\n",
      "  removing _skbuild/macosx-13.0-arm64-3.11/setuptools/bdist.macosx-13.0-arm64/wheel\n",
      "\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.77-cp311-cp311-macosx_13_0_arm64.whl size=585742 sha256=cb162d3fce500057d4b2ad583247dc3e35f13435d8dc766c00e5acf8f59d959f\n",
      "  Stored in directory: /private/var/folders/vc/cgq9zwh170j_k6234t_98kmh0000gn/T/pip-ephem-wheel-cache-azjbuh5w/wheels/e2/67/cb/481cfaabbb5fd5edab627c5b475de63e1b6f7d4d7b678d4d25\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/__pycache__/typing_extensions.cpython-311.pyc\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info/\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/typing_extensions.py\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.1\n",
      "    Uninstalling numpy-1.25.1:\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/bin/f2py\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/bin/f2py3\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/bin/f2py3.11\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/numpy-1.25.1.dist-info/\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/numpy/\n",
      "      Successfully uninstalled numpy-1.25.1\n",
      "  changing mode of /Users/martinpatino/anaconda3/envs/langchain_ai/bin/f2py to 755\n",
      "  changing mode of /Users/martinpatino/anaconda3/envs/langchain_ai/bin/f2py3 to 755\n",
      "  changing mode of /Users/martinpatino/anaconda3/envs/langchain_ai/bin/f2py3.11 to 755\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.1\n",
      "    Uninstalling diskcache-5.6.1:\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/diskcache-5.6.1.dist-info/\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/diskcache/\n",
      "      Successfully uninstalled diskcache-5.6.1\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama-cpp-python 0.1.77\n",
      "    Uninstalling llama-cpp-python-0.1.77:\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/bin/__pycache__/convert-lora-to-ggml.cpython-311.pyc\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/bin/__pycache__/convert.cpython-311.pyc\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/bin/convert-lora-to-ggml.py\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/bin/convert.py\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/libggml_shared.dylib\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/libllama.dylib\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/llama_cpp/\n",
      "      Removing file or directory /Users/martinpatino/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/llama_cpp_python-0.1.77.dist-info/\n",
      "      Successfully uninstalled llama-cpp-python-0.1.77\n",
      "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.77 numpy-1.25.1 typing-extensions-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import All the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/martinpatino/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 1024\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9720.07 MB (+  800.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  800.00 MB\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Loading model,\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    max_tokens=256,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    n_ctx=1024,\n",
    "    verbose=False,\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query=\"What is the BigO?\"\n",
    "docs=docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Data science, machine learning engineering, and data engineering cover different functions within the big data paradigm — an approach wherein huge velocities, varieties, and volumes of structured, unstructured, and semistructured data are being captured, processed, stored, and analyzed using a set of techniques and technologies that are completely novel compared to those that were used in decades past.', metadata={}),\n",
       " Document(page_content='Data science, machine learning engineering, and data engineering cover different functions within the big data paradigm — an approach wherein huge velocities, varieties, and volumes of structured, unstructured, and semistructured data are being captured, processed, stored, and analyzed using a set of techniques and technologies that are completely novel compared to those that were used in decades past.', metadata={}),\n",
       " Document(page_content='terms, MapReduce uses parallel distributed computing to transform big data into data of a manageable size.', metadata={}),\n",
       " Document(page_content='terms, MapReduce uses parallel distributed computing to transform big data into data of a manageable size.', metadata={})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=load_qa_chain(llm, chain_type=\"stuff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Big O (capital Oh) refers to the order of growth of a function's running time or memory usage as the input size increases, notating which part of the algorithm contributes most to the overall performance."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The Big O (capital Oh) refers to the order of growth of a function's running time or memory usage as the input size increases, notating which part of the algorithm contributes most to the overall performance.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
